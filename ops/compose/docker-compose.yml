x-ollama-gpu-deploy: &ollama-gpu-deploy
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]

x-ollama-gpu-env: &ollama-gpu-env
  NVIDIA_VISIBLE_DEVICES: "all"
  NVIDIA_DRIVER_CAPABILITIES: "compute,utility"

services:
  postgres:
    image: postgres:15
    restart: unless-stopped
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: youworker
    ports:
      - "5432:5432"
    volumes:
      - ../../data/postgres:/var/lib/postgresql/data
  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    ports:
      - "6333:6333"
    volumes:
      - ../../data/qdrant:/qdrant/storage

  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "11434:11434"
    deploy: *ollama-gpu-deploy
    volumes:
      - ../../data/ollama:/root/.ollama
    environment:
      <<: *ollama-gpu-env

  mcp_web:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.mcp_web
    command: ["uvicorn", "apps.mcp_servers.web.server:app", "--host", "0.0.0.0", "--port", "7001"]
    restart: unless-stopped
    ports:
      - "7001:7001"

  mcp_semantic:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.mcp_semantic
    command: ["uvicorn", "apps.mcp_servers.semantic.server:app", "--host", "0.0.0.0", "--port", "7002"]
    restart: unless-stopped
    ports:
      - "7002:7002"
    depends_on:
      - qdrant
      - ollama

  mcp_datetime:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.mcp_datetime
    command: ["uvicorn", "apps.mcp_servers.datetime.server:app", "--host", "0.0.0.0", "--port", "7003"]
    restart: unless-stopped
    ports:
      - "7003:7003"

  mcp_ingest:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.mcp_ingest
    command: ["uvicorn", "apps.mcp_servers.ingest.server:app", "--host", "0.0.0.0", "--port", "7004"]
    restart: unless-stopped
    ports:
      - "7004:7004"
    depends_on:
      - qdrant
      - ollama
      - postgres
    environment:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@postgres:5432/youworker
      QDRANT_URL: http://qdrant:6333
      OLLAMA_BASE_URL: http://ollama:11434

  mcp_units:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.mcp_units
    command: ["uvicorn", "apps.mcp_servers.units.server:app", "--host", "0.0.0.0", "--port", "7005"]
    restart: unless-stopped
    ports:
      - "7005:7005"

  mcp_audio:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.mcp_audio
    command: ["uvicorn", "apps.mcp_servers.audio.server:app", "--host", "0.0.0.0", "--port", "7006"]
    environment:
      # TTS Configuration
      TTS_VOICE: ${TTS_VOICE:-it_IT-paola-medium}  # Italian female voice for Piper TTS (smooth, natural)
      TTS_PROVIDER: ${TTS_PROVIDER:-piper}  # TTS provider: piper, coqui, etc.
      TTS_MODEL_DIR: ${TTS_MODEL_DIR:-/app/models/tts}
      
      # STT Configuration
      STT_PROVIDER: ${STT_PROVIDER:-faster-whisper}  # STT provider: faster-whisper, whisper, etc.
      STT_MODEL: ${STT_MODEL:-large-v3}  # Whisper model size
      STT_DEVICE: ${STT_DEVICE:-auto}  # Device: auto, cpu, cuda
      STT_COMPUTE_TYPE: ${STT_COMPUTE_TYPE:-float16}  # Compute type: float16, int8, auto
      STT_VAD_ENABLED: ${STT_VAD_ENABLED:-true}  # Voice Activity Detection
      STT_BEAM_SIZE: ${STT_BEAM_SIZE:-1}  # Beam size for decoding
      STT_MIN_SILENCE_MS: ${STT_MIN_SILENCE_MS:-400}  # Minimum silence for VAD
      STT_CHUNK_SIZE_MS: ${STT_CHUNK_SIZE_MS:-400}  # Chunk size for processing
      
      # Audio Processing
      AUDIO_SAMPLE_RATE: ${AUDIO_SAMPLE_RATE:-24000}  # Default sample rate
      AUDIO_FRAME_MS: ${AUDIO_FRAME_MS:-20}  # Frame size in milliseconds
      
      # Session Management
      SESSION_TTL_SECONDS: ${SESSION_TTL_SECONDS:-300}
      
      # Performance Tuning
      WORKER_THREADS: ${WORKER_THREADS:-4}
      MAX_CONCURRENT_SESSIONS: ${MAX_CONCURRENT_SESSIONS:-100}
    restart: unless-stopped
    ports:
      - "7006:7006"
    volumes:
      - ../../data/models:/app/models:ro
      - ../../data/audio:/app/audio:rw

  api:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.api
      args:
        ENABLE_GPU_TORCH: ${ENABLE_GPU_TORCH:-0}
    environment:
      <<: *ollama-gpu-env
      APP_ENV: production
      API_PORT: 8001
      FRONTEND_ORIGIN: "http://localhost:8000,http://127.0.0.1:8000,http://95.110.228.79:8000,http://93.41.222.40:8000"
      OLLAMA_BASE_URL: http://ollama:11434
      CHAT_MODEL: gpt-oss:latest
      EMBED_MODEL: embeddinggemma:300m
      QDRANT_URL: http://qdrant:6333
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@postgres:5432/youworker
      ROOT_API_KEY: dev-root-key
      MCP_SERVER_URLS: http://mcp_web:7001,http://mcp_semantic:7002,http://mcp_datetime:7003,http://mcp_ingest:7004,http://mcp_units:7005,http://mcp_audio:7006
      INGEST_ACCELERATOR: ${INGEST_ACCELERATOR:-auto}
      INGEST_GPU_DEVICE: ${INGEST_GPU_DEVICE:-cuda}
    restart: unless-stopped
    deploy: *ollama-gpu-deploy
    gpus: "all"
    ports:
      - "8001:8001"
    volumes:
      - ../../examples/ingestion:/data/examples:ro
      - ../../data/uploads:/data/uploads
    depends_on:
      ollama:
        condition: service_started
      qdrant:
        condition: service_started
      postgres:
        condition: service_started
      mcp_web:
        condition: service_started
      mcp_semantic:
        condition: service_started
      mcp_datetime:
        condition: service_started
      mcp_ingest:
        condition: service_started
      mcp_units:
        condition: service_started
      mcp_audio:
        condition: service_started

  frontend:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.frontend
      args:
        NEXT_PUBLIC_API_BASE_URL: ${NEXT_PUBLIC_API_BASE_URL:-}
        NEXT_PUBLIC_API_PORT: ${NEXT_PUBLIC_API_PORT:-8001}
        NEXT_PUBLIC_API_KEY: ${NEXT_PUBLIC_API_KEY:-dev-root-key}
        NEXT_PUBLIC_AUDIO_BASE_URL: ${NEXT_PUBLIC_AUDIO_BASE_URL:-http://localhost:7006}
    environment:
      NEXT_INTERNAL_API_BASE_URL: http://api:8001
      NEXT_PUBLIC_API_KEY: ${NEXT_PUBLIC_API_KEY:-dev-root-key}
      NEXT_PUBLIC_AUDIO_BASE_URL: ${NEXT_PUBLIC_AUDIO_BASE_URL:-http://localhost:7006}
    restart: unless-stopped
    ports:
      - "8000:3000"
    depends_on:
      api:
        condition: service_started
