x-ollama-gpu-deploy: &ollama-gpu-deploy
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]

x-ollama-gpu-env: &ollama-gpu-env
  NVIDIA_VISIBLE_DEVICES: "all"
  NVIDIA_DRIVER_CAPABILITIES: "compute,utility"

services:
  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    ports:
      - "6333:6333"
    volumes:
      - ../../data/qdrant:/qdrant/storage

  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "11434:11434"
    deploy: *ollama-gpu-deploy
    volumes:
      - ../../data/ollama:/root/.ollama
    environment:
      <<: *ollama-gpu-env

  mcp_web:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.mcp_web
    command: ["uvicorn", "apps.mcp_servers.web.server:app", "--host", "0.0.0.0", "--port", "7001"]
    restart: unless-stopped
    ports:
      - "7001:7001"

  mcp_semantic:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.mcp_semantic
    command: ["uvicorn", "apps.mcp_servers.semantic.server:app", "--host", "0.0.0.0", "--port", "7002"]
    restart: unless-stopped
    ports:
      - "7002:7002"
    depends_on:
      - qdrant
      - ollama

  mcp_datetime:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.mcp_datetime
    command: ["uvicorn", "apps.mcp_servers.datetime.server:app", "--host", "0.0.0.0", "--port", "7003"]
    restart: unless-stopped
    ports:
      - "7003:7003"

  api:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.api
      args:
        ENABLE_GPU_TORCH: ${ENABLE_GPU_TORCH:-0}
    environment:
      <<: *ollama-gpu-env
      APP_ENV: production
      API_PORT: 8001
      FRONTEND_ORIGIN: "http://localhost:8000,http://127.0.0.1:8000,http://95.110.228.79:8000,http://93.41.222.40:8000"
      OLLAMA_BASE_URL: http://ollama:11434
      CHAT_MODEL: gpt-oss:latest
      EMBED_MODEL: embeddinggemma:300m
      QDRANT_URL: http://qdrant:6333
      MCP_SERVER_URLS: http://mcp_web:7001,http://mcp_semantic:7002,http://mcp_datetime:7003
      INGEST_ACCELERATOR: ${INGEST_ACCELERATOR:-auto}
      INGEST_GPU_DEVICE: ${INGEST_GPU_DEVICE:-cuda}
    restart: unless-stopped
    deploy: *ollama-gpu-deploy
    gpus: "all"
    ports:
      - "8001:8001"
    volumes:
      - ../../examples/ingestion:/data/examples:ro
    depends_on:
      ollama:
        condition: service_started
      qdrant:
        condition: service_started
      mcp_web:
        condition: service_started
      mcp_semantic:
        condition: service_started
      mcp_datetime:
        condition: service_started

  frontend:
    build:
      context: ../..
      dockerfile: ops/docker/Dockerfile.frontend
      args:
        NEXT_PUBLIC_API_BASE_URL: ${NEXT_PUBLIC_API_BASE_URL:-}
        NEXT_PUBLIC_API_PORT: ${NEXT_PUBLIC_API_PORT:-8001}
    environment:
      NEXT_INTERNAL_API_BASE_URL: http://api:8001
    restart: unless-stopped
    ports:
      - "8000:3000"
    depends_on:
      api:
        condition: service_started
